{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert the following packages before running the code:\n",
    "1/ cv2\n",
    "2/ tqdm\n",
    "3/ densratio\n",
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "''' Import relevant libraries'''\n",
    "import cv2 # using opencv to process image\n",
    "from tqdm import tqdm  # use tqdm to know the process of the iteration\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os         \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from densratio import densratio\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_path = r'C:\\Users\\phuph\\Desktop\\Spiderdatabase\\spidertrainset'\n",
    "csv_path = r'C:\\Users\\phuph\\Desktop\\Spiderdatabase\\spider_csv.csv'\n",
    "#test_path = 'test'\n",
    "\n",
    "size_img = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" From the image, record the label either dangerous or non\"\"\"\n",
    "def record_label(image_name):\n",
    "    label = image_name.split('.')[0]\n",
    "    if label == 'dangerous':\n",
    "        return 1\n",
    "    elif label == 'non':\n",
    "        return 0\n",
    "\n",
    "def create_train():\n",
    "    training_data = []\n",
    "    for img in tqdm(os.listdir(train_path)):\n",
    "        path = os.path.join(train_path, img)\n",
    "        img_data = cv2.imread(path, cv2.IMREAD_COLOR).astype(float)\n",
    "        img_data = cv2.resize(img_data, (size_img, size_img))\n",
    "        training_data.append([img_data, record_label(img) ])\n",
    "    #np.save('train_data.npy', training_data)\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1005/1005 [00:16<00:00, 59.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train = create_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_train = []\n",
    "for i in train:\n",
    "    a = i[0].reshape(-1)\n",
    "    A_train.append(a)\n",
    "A_train = np.asarray(A_train)\n",
    "B_train = np.asarray([i[1] for i in train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phuph\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(A_train.shape[0])\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train0, X_test0, Y_train, Y_test,idx1,idx2 = train_test_split(A_train, B_train,indices, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train1 = scaler.fit_transform(X_train0)\n",
    "X_test1 = scaler.fit_transform(X_test0)\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "X_train = pca.fit_transform(X_train1)\n",
    "X_test= pca.transform(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nY_new_list = []\\nfor i in range(5):\\n    rho_po = random.choice(rho_po_list)\\n    rho_ne = random.choice(rho_ne_list)\\n    Y_temp = Y_train * (np.cumsum(Y_train) <= (1 - rho_po) * sum(Y_train))\\n    s = 1 - (1 - Y_train) * (np.cumsum(1 - Y_train) <= (1 - rho_ne) * sum(1 - Y_train))\\n    Y_temp[Y_train==0] = s[Y_train==0]\\n    Y_new_list.append([Y_temp,rho_po,rho_ne])\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Induce CCN flip rate'''\n",
    "\n",
    "import random\n",
    "rho_po_list = [0.1,0.2,0.3,0.4,0.4]\n",
    "rho_ne_list = [0.3,0.1,0.4,0.1,0.4]\n",
    "Y_new_list = []\n",
    "for i in range(5):\n",
    "    rho_po = rho_po_list[i]\n",
    "    rho_ne = rho_ne_list[i]\n",
    "    Y_temp = np.copy(Y_train)\n",
    "    for j in range(len(Y_temp)):\n",
    "        if Y_train[j] ==1:\n",
    "            temp = random.random()\n",
    "            if temp < rho_po:\n",
    "                Y_temp[j] = 0\n",
    "        else:\n",
    "            t = random.random()\n",
    "            if t < rho_ne:\n",
    "                Y_temp[j] = 1\n",
    "    Y_new_list.append([Y_temp,rho_po,rho_ne])\n",
    "'''\n",
    "Y_new_list = []\n",
    "for i in range(5):\n",
    "    rho_po = random.choice(rho_po_list)\n",
    "    rho_ne = random.choice(rho_ne_list)\n",
    "    Y_temp = Y_train * (np.cumsum(Y_train) <= (1 - rho_po) * sum(Y_train))\n",
    "    s = 1 - (1 - Y_train) * (np.cumsum(1 - Y_train) <= (1 - rho_ne) * sum(1 - Y_train))\n",
    "    Y_temp[Y_train==0] = s[Y_train==0]\n",
    "    Y_new_list.append([Y_temp,rho_po,rho_ne])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Import R and KLIEP'''\n",
    "\n",
    "import rpy2\n",
    "import rpy2.robjects as robjects\n",
    "r = robjects.r\n",
    "#from rpy2.robjects.packages import importr\n",
    "#utils = importr(\"densratio\")\n",
    "import rpy2.robjects.numpy2ri as numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "\n",
    "robjects.r('''\n",
    "           f <- function(t,t1) {\n",
    "\n",
    "                    library(densratio)\n",
    "                    dens <- densratio(x = t, y = t1, method = \"KLIEP\")\n",
    "                    result <-dens$compute_density_ratio(t1)\n",
    "            }\n",
    "            ''')\n",
    "kliep_check = robjects.globalenv['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Estimate the noise rate###\n",
    "def KLIEP_e(X,Y):\n",
    "    e_list=[]\n",
    "    X_rho_po= X[Y ==1]\n",
    "    X_rho_ne= X[Y ==0]\n",
    "    result_po= kliep_check(X_rho_po,X)\n",
    "    result_ne= kliep_check(X_rho_ne,X)\n",
    "    Po = np.asarray(result_po)\n",
    "    Ne = np.asarray(result_ne)\n",
    "    n = len(X)\n",
    "    py_po = sum(1 for i in Y if i==1)/n\n",
    "    py_ne = sum(1 for i in Y if i==0)/n\n",
    "    Po_n = min(Po*py_po)\n",
    "    Ne_n = min(Ne*py_ne)\n",
    "    e_list.append([Ne_n,Po_n])\n",
    "    return e_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Using KLIEP to estimate betas'''\n",
    "\n",
    "def KLIEP_betas(X,Y):\n",
    "    e_list=[]\n",
    "    X_rho_po= X[Y ==1]\n",
    "    X_rho_ne= X[Y ==0]\n",
    "    result_po= kliep_check(X_rho_po,X)\n",
    "    result_ne= kliep_check(X_rho_ne,X)\n",
    "    Po = np.asarray(result_po)\n",
    "    Ne = np.asarray(result_ne)\n",
    "    n = len(X)\n",
    "    py_po = sum(1 for i in Y if i==1)/n\n",
    "    py_ne = sum(1 for i in Y if i==0)/n\n",
    "    Po_n = min(Po*py_po)\n",
    "    Ne_n = min(Ne*py_ne)\n",
    "    betas = np.ones((n,), dtype = None)\n",
    "    for i in range(n):\n",
    "        if Y[i] == 0:\n",
    "            if Po[i] == 0:\n",
    "                betas[i] = 0\n",
    "            else:\n",
    "                betas[i] = (Po[i]*py_po - Po_n)/ ((1-Po_n-Ne_n)*Po[i]*py_po)\n",
    "        else: \n",
    "            if Ne[i] == 0:\n",
    "                betas[i] =0\n",
    "            else:\n",
    "                betas[i] = (Ne[i]*py_ne - Ne_n)/ ((1-Po_n-Ne_n)*Ne[i]*py_ne)\n",
    "\n",
    "    return betas    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 20 \n",
      "Number of classes: 2\n",
      "Building model...\n"
     ]
    }
   ],
   "source": [
    "''' Neural networks model'''\n",
    "\n",
    "dims = X_train.shape[1]\n",
    "n_y = 2\n",
    "print( 'Number of features: %d ' %dims)\n",
    "print( 'Number of classes: %d' %n_y)\n",
    "print(\"Building model...\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(80, input_shape=(dims,)))\n",
    "model.add(Dense(60, input_shape=(dims,)))\n",
    "model.add(Dense(40, input_shape=(dims,)))\n",
    "model.add(Dense(n_y, input_shape=(dims,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',sample_weight_mode=None,metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0\n",
      " 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
      " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
      " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0\n",
      " 0 1 0 0 1 0 1 1 1 1 1 0 0]\n",
      "Epoch 1/1\n",
      "494/494 [==============================] - ETA: 0s - loss: 0.6109 - acc: 0.687 - ETA: 0s - loss: 0.4711 - acc: 0.750 - ETA: 0s - loss: 0.3350 - acc: 0.822 - ETA: 0s - loss: 0.3173 - acc: 0.868 - ETA: 0s - loss: 0.3084 - acc: 0.864 - ETA: 0s - loss: 0.2411 - acc: 0.898 - ETA: 0s - loss: 0.2378 - acc: 0.906 - ETA: 0s - loss: 0.2188 - acc: 0.911 - ETA: 0s - loss: 0.1930 - acc: 0.922 - 0s - loss: 0.1917 - acc: 0.9231     \n",
      " 32/201 [===>..........................] - ETA: 0s[1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
      " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
      " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
      " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0]\n",
      "Epoch 1/1\n",
      "518/518 [==============================] - ETA: 0s - loss: 0.2236 - acc: 0.937 - ETA: 0s - loss: 0.1896 - acc: 0.937 - ETA: 0s - loss: 0.1674 - acc: 0.942 - ETA: 0s - loss: 0.1530 - acc: 0.941 - ETA: 0s - loss: 0.1256 - acc: 0.953 - ETA: 0s - loss: 0.1250 - acc: 0.956 - ETA: 0s - loss: 0.1224 - acc: 0.957 - 0s - loss: 0.1224 - acc: 0.9575     \n",
      " 32/201 [===>..........................] - ETA: 0s[1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 1]\n",
      "Epoch 1/1\n",
      "104/104 [==============================] - ETA: 0s - loss: 0.6418 - acc: 0.906 - ETA: 0s - loss: 0.3458 - acc: 0.937 - 0s - loss: 0.3195 - acc: 0.9423     \n",
      " 32/201 [===>..........................] - ETA: 0s[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 0 1 0 1 0 0 1 1]\n",
      "Epoch 1/1\n",
      "381/381 [==============================] - ETA: 0s - loss: 0.3369 - acc: 0.937 - ETA: 0s - loss: 0.1524 - acc: 0.976 - ETA: 0s - loss: 0.2339 - acc: 0.958 - ETA: 0s - loss: 0.3599 - acc: 0.947 - 0s - loss: 0.3182 - acc: 0.9554     \n",
      " 32/201 [===>..........................] - ETA: 0s[0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 1]\n",
      "Epoch 1/1\n",
      "30/30 [==============================] - 0s - loss: 0.2980 - acc: 0.8333\n",
      " 32/201 [===>..........................] - ETA: 0s[0, 0.62189054726368154, 0.1, 0.3]\n",
      "[0, 0.57213930348258701, 0.2, 0.1]\n",
      "[0, 0.60199004975124382, 0.3, 0.4]\n",
      "[0, 0.57213930348258701, 0.4, 0.1]\n",
      "[0, 0.58208955223880599, 0.4, 0.4]\n"
     ]
    }
   ],
   "source": [
    "# CCN - Cheng17 + random using LR as noise estimation##\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l2\")\n",
    "\n",
    "accuracy_list =[]\n",
    "for i in Y_new_list:\n",
    "    r = [0.1, 0.2, 0.3, 0.4]\n",
    "    rho_po_max = i[1]\n",
    "    rho_ne_max = i[2]\n",
    "    n = len(X_train)\n",
    "    clf.fit(X_train,i[0])\n",
    "    e = clf.predict_proba(X_train)\n",
    "    eta_po = np.asarray([i[1] for i in e])\n",
    "    St = []\n",
    "    St_n=[]\n",
    "    distill_ind = []\n",
    "\n",
    "    for t in range(n):\n",
    "        if eta_po[t] > ((1+rho_ne_max)/2):\n",
    "            St.append([X_train[t],1])\n",
    "            distill_ind.append([idx1[t],1])\n",
    "\n",
    "        elif eta_po [t]  < ((1-rho_po_max)/2):\n",
    "            St.append([X_train[t],0])\n",
    "            distill_ind.append([idx1[t],0])\n",
    "        else: \n",
    "            St_n.append([X_train[t],Y_train[t]])\n",
    "    \n",
    "    q = 0\n",
    "    while (q<20):\n",
    "        z = random.randint(0,len(St_n))\n",
    "        St.append(St_n[z])\n",
    "        q = q+1\n",
    "    \n",
    "      \n",
    "    St_x =np.asarray([i[0] for i in St])\n",
    "    St_y =np.asarray([i[1] for i in St])\n",
    "    \n",
    "    print(St_y)\n",
    "    \n",
    "    count =0\n",
    "    sample_weight = KLIEP_betas(St_x,St_y)\n",
    "    y= np_utils.to_categorical(St_y, 2)\n",
    "    model.fit(St_x,y,sample_weight=sample_weight,verbose=1,epochs=1)\n",
    "    pred = model.predict_classes(X_test)\n",
    "    a = accuracy_score(Y_test,pred)\n",
    "    accuracy_list.append([count,a,rho_po_max,rho_ne_max])\n",
    "    count = count +1\n",
    "    \n",
    "for i in accuracy_list:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0\n",
      " 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1\n",
      " 1 0 0 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 1 0 1 1\n",
      " 1 0 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 0 1 0 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1\n",
      " 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
      " 1 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1\n",
      " 1 1 1 0 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      "Epoch 1/1\n",
      "504/504 [==============================] - ETA: 0s - loss: 1.8155 - acc: 0.562 - ETA: 0s - loss: 0.6666 - acc: 0.804 - ETA: 0s - loss: 0.5135 - acc: 0.834 - ETA: 0s - loss: 0.3913 - acc: 0.865 - ETA: 0s - loss: 0.3418 - acc: 0.879 - 0s - loss: 0.3134 - acc: 0.8889     \n",
      " 32/201 [===>..........................] - ETA: 0s[1 0 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 0 1\n",
      " 0 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
      " 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0\n",
      " 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1\n",
      " 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
      " 1 1 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
      " 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0\n",
      " 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
      " 0 1 1 0 1 1 1 0 0 0]\n",
      "Epoch 1/1\n",
      "528/528 [==============================] - ETA: 0s - loss: 0.3227 - acc: 0.843 - ETA: 0s - loss: 0.2467 - acc: 0.898 - ETA: 0s - loss: 0.2243 - acc: 0.919 - ETA: 0s - loss: 0.1947 - acc: 0.928 - ETA: 0s - loss: 0.1731 - acc: 0.937 - ETA: 0s - loss: 0.1597 - acc: 0.939 - 0s - loss: 0.1552 - acc: 0.9413     \n",
      " 32/201 [===>..........................] - ETA: 0s[1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 1 1 0\n",
      " 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0\n",
      " 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0]\n",
      "Epoch 1/1\n",
      "114/114 [==============================] - ETA: 0s - loss: 0.5521 - acc: 0.843 - 0s - loss: 0.3210 - acc: 0.8947     \n",
      " 32/201 [===>..........................] - ETA: 0s[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
      " 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 1 1]\n",
      "Epoch 1/1\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.4637 - acc: 0.750 - ETA: 0s - loss: 0.7885 - acc: 0.895 - ETA: 0s - loss: 0.7509 - acc: 0.895 - ETA: 0s - loss: 0.5925 - acc: 0.923 - ETA: 0s - loss: 0.6053 - acc: 0.927 - 0s - loss: 0.5957 - acc: 0.9284     \n",
      " 32/201 [===>..........................] - ETA: 0s[0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 1]\n",
      "Epoch 1/1\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.8088 - acc: 0.750 - 0s - loss: 0.7426 - acc: 0.7000     \n",
      " 32/201 [===>..........................] - ETA: 0s[0, 0.62189054726368154, 0.1, 0.3]\n",
      "[0, 0.57213930348258701, 0.2, 0.1]\n",
      "[0, 0.58706467661691542, 0.3, 0.4]\n",
      "[0, 0.61194029850746268, 0.4, 0.1]\n",
      "[0, 0.61194029850746268, 0.4, 0.4]\n"
     ]
    }
   ],
   "source": [
    "### CCN - Cheng17 auto + act   ###\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l2\")\n",
    "\n",
    "accuracy_list =[]\n",
    "for i in Y_new_list:\n",
    "    r = [0.1, 0.2, 0.3, 0.4]\n",
    "    rho_po_max = i[1]\n",
    "    rho_ne_max = i[2]\n",
    "    n = len(X_train)\n",
    "     \n",
    "    clf.fit(X_train,i[0])\n",
    "    e = clf.predict_proba(X_train)\n",
    "    eta_po = np.asarray([i[1] for i in e])\n",
    "    d = clf.decision_function(X_train)\n",
    "\n",
    "    St = []\n",
    "    St_n=[]\n",
    "    St_d=[]\n",
    "    distill_ind = []\n",
    "\n",
    "    for t in range(n):\n",
    "        if eta_po[t] > ((1+rho_ne_max)/2):\n",
    "            St.append([X_train[t],1])\n",
    "            distill_ind.append([idx1[t],1])\n",
    "\n",
    "        elif eta_po [t]  < ((1-rho_po_max)/2):\n",
    "            St.append([X_train[t],0])\n",
    "            distill_ind.append([idx1[t],0])\n",
    "        else: \n",
    "            St_n.append([X_train[t],Y_train[t]])\n",
    "            St_d.append(d[t])\n",
    "    dist = np.abs(St_d)\n",
    "    a = dist.argsort()[:30]\n",
    "    for z in a:\n",
    "        St.append(St_n[z])\n",
    "    \n",
    "    St_x =np.asarray([i[0] for i in St])\n",
    "    St_y =np.asarray([i[1] for i in St])\n",
    "    \n",
    "    print(St_y)\n",
    "    count =0\n",
    "    sample_weight = KLIEP_betas(St_x,St_y)\n",
    "    y= np_utils.to_categorical(St_y, 2)\n",
    "    model.fit(St_x,y,sample_weight=sample_weight,verbose=1,epochs=1)\n",
    "    pred = model.predict_classes(X_test)\n",
    "    a = accuracy_score(Y_test,pred)\n",
    "    accuracy_list.append([count,a,rho_po_max,rho_ne_max])\n",
    "    count = count +1\n",
    "    \n",
    "for i in accuracy_list:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ILN - Cheng17 + rd using KLIEP\"\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "accuracy_list =[]\n",
    "df_noise = pd.read_csv(csv_path)\n",
    "C_train = np.asarray(df_noise[\"Rater label\"])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index,test_index in skf.split(A_train,C_train,B_train):\n",
    "    X_train0, X_test0 = A_train[train_index], A_train[test_index]\n",
    "    Y_train, Y_test = C_train[train_index], C_train[test_index]\n",
    "    BB_train, BB_test = B_train[train_index], B_train[test_index]\n",
    "    idx1= train_index\n",
    "    X_train1 = scaler.fit_transform(X_train0)\n",
    "    X_test1 = scaler.fit_transform(X_test0)\n",
    "\n",
    "    X_train = pca.fit_transform(X_train1)\n",
    "    X_test= pca.transform(X_test1)\n",
    "\n",
    "    r = [0.1,0.15, 0.2, 0.25, 0.3,0.35, 0.4, 0.45]\n",
    "    #rho_po_max = 0.2\n",
    "    #rho_ne_max = 0.45\n",
    "    rho_po_max = random.choice(rho_po_list)\n",
    "    rho_ne_max = random.choice(rho_ne_list)\n",
    "    X_rho_po= X_train[Y_train ==1]\n",
    "    result_po= kliep_check(X_rho_po,X_train)\n",
    "    Po = np.asarray(result_po)\n",
    "    n = len(X_train)\n",
    "    py_po = sum(1 for z in Y_train if z==1)/n\n",
    "    eta_po = Po*py_po\n",
    "\n",
    "    St = []\n",
    "    St_n=[]\n",
    "    distill_ind = []\n",
    "\n",
    "    for t in range(n):\n",
    "        if eta_po[t] > ((1+rho_ne_max)/2):\n",
    "            St.append([X_train[t],1])\n",
    "            distill_ind.append([idx1[t],1])\n",
    "\n",
    "        elif eta_po [t]  < ((1-rho_po_max)/2):\n",
    "            St.append([X_train[t],0])\n",
    "            distill_ind.append([idx1[t],0])\n",
    "        else: \n",
    "            St_n.append([X_train[t],BB_train[t]])\n",
    "\n",
    "\n",
    "    q = 0\n",
    "    while (q<20):\n",
    "        z = random.randint(0,len(St_n))\n",
    "        St.append(St_n[z])\n",
    "        q = q+1\n",
    "\n",
    "    St_x =np.asarray([t[0] for t in St])\n",
    "    St_y =np.asarray([t[1] for t in St])\n",
    "    print(St_y)\n",
    "    sample_weight = KLIEP_betas(St_x,St_y)\n",
    "    y= np_utils.to_categorical(St_y, 2)\n",
    "    model.fit(St_x,y,sample_weight=sample_weight,verbose=1,epochs=1)\n",
    "    pred = model.predict_classes(X_test)\n",
    "    a = accuracy_score(Y_test,pred)\n",
    "    accuracy_list.append([a])\n",
    "\n",
    "print(np.mean(accuracy_list,axis=0),np.std(accuracy_list,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "106/106 [==============================] - ETA: 0s - loss: 0.9479 - acc: 0.781 - 0s - loss: 0.4930 - acc: 0.8585     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "69/69 [==============================] - ETA: 0s - loss: 2.0219 - acc: 0.625 - 0s - loss: 1.0011 - acc: 0.8116     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.0559 - acc: 0.562 - 0s - loss: 0.7519 - acc: 0.7015     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "552/552 [==============================] - ETA: 0s - loss: 1.2589 - acc: 0.437 - ETA: 0s - loss: 0.5248 - acc: 0.750 - ETA: 0s - loss: 0.3996 - acc: 0.804 - ETA: 0s - loss: 0.3944 - acc: 0.812 - ETA: 0s - loss: 0.3473 - acc: 0.837 - 0s - loss: 0.3183 - acc: 0.8551     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "303/303 [==============================] - ETA: 0s - loss: 0.1579 - acc: 0.937 - ETA: 0s - loss: 0.2155 - acc: 0.943 - ETA: 0s - loss: 0.1713 - acc: 0.941 - 0s - loss: 0.1704 - acc: 0.9406     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "59/59 [==============================] - ETA: 0s - loss: 5.4879 - acc: 0.125 - 0s - loss: 3.4132 - acc: 0.3051     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "103/103 [==============================] - ETA: 0s - loss: 2.0209 - acc: 0.281 - 0s - loss: 0.9070 - acc: 0.5825     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "236/236 [==============================] - ETA: 0s - loss: 1.3719 - acc: 0.625 - ETA: 0s - loss: 0.9584 - acc: 0.775 - 0s - loss: 0.7215 - acc: 0.8263     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "471/471 [==============================] - ETA: 0s - loss: 0.1137 - acc: 1.000 - ETA: 0s - loss: 0.2486 - acc: 0.956 - ETA: 0s - loss: 0.2056 - acc: 0.968 - ETA: 0s - loss: 0.1958 - acc: 0.971 - 0s - loss: 0.1685 - acc: 0.9766     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "167/167 [==============================] - ETA: 0s - loss: 1.0402 - acc: 0.500 - ETA: 0s - loss: 0.4907 - acc: 0.793 - 0s - loss: 0.4709 - acc: 0.8024     \n",
      " 32/100 [========>.....................] - ETA: 0s[ 0.53725743] [ 0.10461637]\n"
     ]
    }
   ],
   "source": [
    "### ILN - Cheng17 + random using LR  ###\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "accuracy_list =[]\n",
    "df_noise = pd.read_csv(csv_path)\n",
    "C_train = np.asarray(df_noise[\"Rater label\"])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index,test_index in skf.split(A_train,C_train,B_train):\n",
    "    x_train0, x_test0 = A_train[train_index], A_train[test_index]\n",
    "    y_train, y_test = C_train[train_index], C_train[test_index]\n",
    "    BB_train, BB_test = B_train[train_index], B_train[test_index]\n",
    "    idx1= train_index\n",
    "    x_train1 = scaler.fit_transform(x_train0)\n",
    "    x_test1 = scaler.fit_transform(x_test0)\n",
    "\n",
    "    x_train = pca.fit_transform(x_train1)\n",
    "    x_test= pca.transform(x_test1)\n",
    "\n",
    "    r = [0.1,0.15, 0.2, 0.25, 0.3,0.35, 0.4, 0.45]\n",
    "    rho_po_max = random.choice(rho_po_list)\n",
    "    rho_ne_max = random.choice(rho_ne_list)\n",
    "    n = len(x_train)\n",
    "    clf.fit(x_train,y_train)\n",
    "    e = clf.predict_proba(x_train)\n",
    "    eta_po = np.asarray([i[1] for i in e])\n",
    "    \n",
    "    St = []\n",
    "    St_n=[]\n",
    "    distill_ind = []\n",
    "\n",
    "    for t in range(n):\n",
    "        if eta_po[t] > ((1+rho_ne_max)/2):\n",
    "            St.append([x_train[t],1])\n",
    "            distill_ind.append([idx1[t],1])\n",
    "\n",
    "        elif eta_po [t]  < ((1-rho_po_max)/2):\n",
    "            St.append([x_train[t],0])\n",
    "            distill_ind.append([idx1[t],0])\n",
    "        else: \n",
    "            St_n.append([x_train[t],BB_train[t]])\n",
    "\n",
    "    \n",
    "    q = 0\n",
    "    while (q<30):\n",
    "        z = random.randint(0,len(St_n))\n",
    "        St.append(St_n[z])\n",
    "        q = q+1\n",
    "    \n",
    "    St_x =np.asarray([t[0] for t in St])\n",
    "    St_y =np.asarray([t[1] for t in St])\n",
    "    #print(St_y)\n",
    "    sample_weight = KLIEP_betas(St_x,St_y)\n",
    "    y= np_utils.to_categorical(St_y, 2)\n",
    "    model.fit(St_x,y,sample_weight=sample_weight,verbose=1,epochs=1)\n",
    "    pred = model.predict_classes(x_test)\n",
    "    a = accuracy_score(y_test,pred)\n",
    "    accuracy_list.append([a])\n",
    "\n",
    "print(np.mean(accuracy_list,axis=0),np.std(accuracy_list,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.5743 - acc: 0.875 - ETA: 0s - loss: 0.2667 - acc: 0.960 - ETA: 0s - loss: 0.2312 - acc: 0.977 - ETA: 0s - loss: 0.1896 - acc: 0.984 - ETA: 0s - loss: 0.1759 - acc: 0.987 - 0s - loss: 0.1677 - acc: 0.9883     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "193/193 [==============================] - ETA: 0s - loss: 2.5657 - acc: 0.375 - ETA: 0s - loss: 1.1959 - acc: 0.710 - 0s - loss: 0.9230 - acc: 0.7824     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "27/27 [==============================] - 0s - loss: 0.3918 - acc: 0.7778\n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "176/176 [==============================] - ETA: 0s - loss: 1.3231 - acc: 0.593 - ETA: 0s - loss: 0.6176 - acc: 0.812 - 0s - loss: 0.6198 - acc: 0.8352     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0034 - acc: 1.000 - 0s - loss: 0.0581 - acc: 0.9595     \n",
      " 32/101 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "431/431 [==============================] - ETA: 0s - loss: 5.9649 - acc: 0.218 - ETA: 0s - loss: 2.1910 - acc: 0.677 - ETA: 0s - loss: 1.4042 - acc: 0.793 - ETA: 0s - loss: 1.0495 - acc: 0.834 - ETA: 0s - loss: 0.7789 - acc: 0.875 - ETA: 0s - loss: 0.6215 - acc: 0.899 - 0s - loss: 0.6071 - acc: 0.8979     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "70/70 [==============================] - ETA: 0s - loss: 3.1409 - acc: 0.093 - 0s - loss: 1.6644 - acc: 0.5429     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "235/235 [==============================] - ETA: 0s - loss: 1.2010 - acc: 0.781 - ETA: 0s - loss: 0.6696 - acc: 0.882 - ETA: 0s - loss: 0.5443 - acc: 0.888 - 0s - loss: 0.5278 - acc: 0.8894     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "681/681 [==============================] - ETA: 0s - loss: 0.4313 - acc: 0.906 - ETA: 0s - loss: 0.2608 - acc: 0.882 - ETA: 0s - loss: 0.2211 - acc: 0.906 - ETA: 0s - loss: 0.1867 - acc: 0.925 - ETA: 0s - loss: 0.1616 - acc: 0.935 - ETA: 0s - loss: 0.1460 - acc: 0.941 - ETA: 0s - loss: 0.1314 - acc: 0.950 - 0s - loss: 0.1343 - acc: 0.9471     \n",
      " 32/100 [========>.....................] - ETA: 0sEpoch 1/1\n",
      "67/67 [==============================] - ETA: 0s - loss: 1.4609 - acc: 0.656 - 0s - loss: 0.7440 - acc: 0.8209     \n",
      " 32/100 [========>.....................] - ETA: 0s[ 0.53130693] [ 0.08355827]\n"
     ]
    }
   ],
   "source": [
    "### ILN - Cheng17 auto + act using LR  ###\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l2\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "accuracy_list =[]\n",
    "df_noise = pd.read_csv(csv_path)\n",
    "C_train = np.asarray(df_noise[\"Rater label\"])\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "for train_index,test_index in skf.split(A_train,C_train, B_train):\n",
    "    x_train0, x_test0 = A_train[train_index], A_train[test_index]\n",
    "    y_train, y_test = C_train[train_index], C_train[test_index]\n",
    "    BB_train, BB_test = B_train[train_index], B_train[test_index]\n",
    "    idx1= train_index\n",
    "    x_train1 = scaler.fit_transform(x_train0)\n",
    "    x_test1 = scaler.fit_transform(x_test0)\n",
    "\n",
    "    x_train = pca.fit_transform(x_train1)\n",
    "    x_test= pca.transform(x_test1)\n",
    "\n",
    "    r = [0.1,0.15, 0.2, 0.25, 0.3,0.35, 0.4, 0.45]\n",
    "    rho_po_max = random.choice(rho_po_list)\n",
    "    rho_ne_max = random.choice(rho_ne_list)\n",
    "    n = len(x_train)\n",
    "    clf.fit(x_train,y_train)\n",
    "    e = clf.predict_proba(x_train)\n",
    "    eta_po = np.asarray([i[1] for i in e])\n",
    "    d = clf.decision_function(x_train)\n",
    "        \n",
    "    \n",
    "    St = []\n",
    "    St_n=[]\n",
    "    distill_ind = []\n",
    "    St_d=[]\n",
    "\n",
    "    for t in range(n):\n",
    "        if eta_po[t] > ((1+rho_ne_max)/2):\n",
    "            St.append([x_train[t],1])\n",
    "            distill_ind.append([idx1[t],1])\n",
    "\n",
    "        elif eta_po [t]  < ((1-rho_po_max)/2):\n",
    "            St.append([x_train[t],0])\n",
    "            distill_ind.append([idx1[t],0])\n",
    "        else: \n",
    "            St_n.append([x_train[t],BB_train[t]])\n",
    "            St_d.append(d[t])\n",
    "     \n",
    "    dist = np.abs(St_d)\n",
    "    a = dist.argsort()[:20]\n",
    "    for z in a:\n",
    "        St.append(St_n[z])\n",
    "  \n",
    "\n",
    "    St_x =np.asarray([t[0] for t in St])\n",
    "    St_y =np.asarray([t[1] for t in St])\n",
    "    #print(St_y)\n",
    "    sample_weight = KLIEP_betas(St_x,St_y)\n",
    "    y= np_utils.to_categorical(St_y, 2)\n",
    "    model.fit(St_x,y,sample_weight=sample_weight,verbose=1,epochs=1)\n",
    "    pred = model.predict_classes(x_test)\n",
    "    a = accuracy_score(y_test,pred)\n",
    "    accuracy_list.append([a])\n",
    "\n",
    "print(np.mean(accuracy_list,axis=0),np.std(accuracy_list,axis=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
